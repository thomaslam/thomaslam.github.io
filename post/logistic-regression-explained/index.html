<!DOCTYPE html>
<html
  class=""
  lang="en-us"
  prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#"
>
  <head>
    <meta charset="utf-8" />

    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="description" content="" />
<meta name="HandheldFriendly" content="True" />
<meta name="MobileOptimized" content="320" />
<meta name="viewport" content="width=device-width, initial-scale=1" />


<meta name="keywords" content="">


<meta property="og:type" content="article" />
<meta property="og:description" content="" />
<meta property="og:title" content="Logistic regression explained" />
<meta property="og:site_name" content="Thomas Lam" />
<meta property="og:image" content="" />
<meta property="og:image:type" content="image/jpeg" />
<meta property="og:image:width" content="" />
<meta property="og:image:height" content="" />
<meta property="og:url" content="https://thomaslam.github.io/post/logistic-regression-explained/" />
<meta property="og:locale" content="en-us" />
<meta property="article:published_time" content="2019-04-26
" /> <meta property="article:modified_time" content="2019-04-26
" />






    <title>Logistic regression explained</title>
    <link rel="canonical" href="https://thomaslam.github.io/post/logistic-regression-explained/" />


    <link
  rel="stylesheet"
  href="https://unpkg.com/tachyons@4.11.1/css/tachyons.min.css"
/>

<link rel="stylesheet" href="https://thomaslam.github.io/css/style.css" />

<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/highlightjs@9.12.0/styles/github-gist.css"
/>


<script type="application/javascript">
var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
var doNotTrack = (dnt == "1" || dnt == "yes");
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	if (window.sessionStorage) {
		var GA_SESSION_STORAGE_KEY = 'ga:clientId';
		ga('create', 'UA-XXXXXXXXX-X', {
	    'storage': 'none',
	    'clientId': sessionStorage.getItem(GA_SESSION_STORAGE_KEY)
	   });
	   ga(function(tracker) {
	    sessionStorage.setItem(GA_SESSION_STORAGE_KEY, tracker.get('clientId'));
	   });
   }
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" href="/apple-touch-icon.png" />
  </head>


<body
  lang="en-us"
  class="sans-serif w-90 w-80-m w-60-ns center mv2 mv5-ns"
  itemscope
  itemtype="http://schema.org/Article"
>
  
  <span class="b">/ </span>
  <a href="https://thomaslam.github.io/" class="b bb bw1 pb1 no-underline black">Thomas Lam</a>
  <span class="b"> / </span>
  <a href="/post" class="b bb bw1 pb1 no-underline black">blog</a>

  <section id="main" class="mt5">
    <h1 itemprop="name" id="title">Logistic regression explained</h1>
    <span class="f6 gray">April 26, 2019</span>

      <article itemprop="articleBody" id="content" class="w-90 lh-copy">
        

<p><strong>Logistic regression</strong> is one of the most famous ML models used in supervised learning settings. Despite the term &ldquo;regression&rdquo; in its name, this model is mostly used for classification tasks. An important distinction between logistic regression and another very popular supervised learning model, <strong>linear regression</strong>, is that the response variable is categorical for the former but continuous for the latter.</p>

<p>Most ML researchers use this model as a baseline in many supervised learning tasks despite its simplicity. In fact, for most practical applications, logistic regression turns out to be the best performing model even compared to more complex deep learning models. For example, it is the best model <a href="/pdfs/Building_an_NCAA_mens_basketball_predictive_model.pdf">to predict outcomes in NCAA mens basketball</a> in a data science competition a few years back.</p>

<h1 id="definition">Definition</h1>

<p>Standard logistic regression is used for binary classification tasks although it can be extended to multiclass settings (&gt; 2 labels). Here&rsquo;s its formal definition:</p>

<blockquote>
<p>Given $m$ training examples $\{ (x^{(1)}, y^{(1)}), \thinspace (x^{(2)}, y^{(2)}), \thinspace&hellip;\thinspace, (x^{(m)}, y^{(m)}) \}$, logistic regression model outputs corresponding predicted values $\{ \hat{y}^{(1)}, \thinspace \hat{y}^{(2)}, \thinspace&hellip;\thinspace, \hat{y}^{(m)} \}$ where $0 \leq \hat{y}^{(i)} \leq 1$ can be interpreted as probability of example (i) belonging to class 1 (i.e. $y^{(i)} = 1$)</p>

<p>$$ \hat{y}^{(i)} = P(y^{(i)} = 1 \thinspace | \thinspace x^{(i)}) = \sigma (w^Tx^{(i)} + b)$$</p>

<p>where $\sigma$ is sigmoid function (see below), $w$, $b$ are model parameters such that $w \in \mathbb{R}^{n_{x}}$, $b \in \mathbb{R}$, each training example is represented by $n_x$-dimentional vector $x^{(i)}$, $y$ can either be 0 or 1 to indicate two classes that each data case can fall into</p>
</blockquote>

<h1 id="sigmoid-function">Sigmoid function</h1>

<p>Sigmoid function, denoted by $\sigma$, is used as <a href="https://en.wikipedia.org/wiki/Activation_function">activation function</a> for logistic regression. It has this following form for a real-valued input z:</p>

<p>$$
\sigma (z) = \frac{1}{1 + e^{-z}}
$$</p>

<p>Here&rsquo;s its graphical representation</p>

<p><img src="/images/sigmoid.png" alt="Sigmoid function" /></p>

<p>Clearly, sigmoid function outputs are bounded between 0 and 1. These outputs can be interpreted as probabilities of data cases belonging to class $y = 1$.</p>

<p>In other words, for a given data case (i), if its predicted $\hat{y}^{(i)} = \sigma (z^{(i)}) = \sigma (w^Tx^{(i)} + b) &gt; 0.5$, then the model predicts that this data case belongs to class $y = 1$</p>

<h1 id="how-is-logistic-regression-fit-to-training-data">How is logistic regression fit to training data?</h1>

<p>Given a <strong>cost function</strong> (also called <strong>objective function</strong>) of $w$ and $b$, the logistic regression model&rsquo;s goal is to find suitable values of $w$ and $b$ such that this cost function&rsquo;s value is as small as possible.</p>

<p>This cost function is not unique to logistic regression. Generally, all (supervised) ML models have associated cost functions as functions of the model parameters. <strong>A cost function quantifies how well a model is performing against the given training data set</strong>, i.e. how far off our predicted response values $\hat{y}^{(i)}$&rsquo;s are from our ground truth response labels $y^{(i)}$&rsquo;s.</p>

<p>So what does this cost function look like for logistic regression?</p>

<p>For a single example (i), we have
$$
\ell (y, \hat{y}) = - (y * log(\hat{y}) + (1-y) * log(1 - \hat{y}))
$$</p>

<p><em>Side note</em>: this cost function is also known as <a href="https://en.wikipedia.org/wiki/Cross_entropy"><strong>cross entropy loss</strong></a></p>

<p>Thus the overal cost function over all training examples is just the average of these individual losses</p>

<p>$$
J(w, b) = \frac{-1}{m} \sum_{i=1}^{m}y * log(\hat{y}) + (1-y) * log(1 - \hat{y})
$$</p>

<p>So how does logistic regression model find values $w$, $b$ to minimize the value of this cost function?</p>

<p>Well, through a training algorithm called <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>.</p>

<p>Those with Calculus background will find this idea familiar: the minimum of the objective function is (hopefully) found by iteratively lowering its value through updating its model parameters in the direction of the cost function&rsquo;s gradient (or partial derivatives) w.r.t. these parameters.</p>

<p>Details about how to mathematically derive this gradient is left in <a href="https://thomaslam.github.io/post/deriving-cross-entropy-loss-gradient/">this post</a></p>

<p>This iterative process is bounded by the number of iterations specified by the model builder or when the difference between cost function values in consecutive steps satisfies some threshold.</p>

<p>This process can also be fine-tuned by a hyperparameter called the <strong>learning rate</strong> (usually denoted by $\alpha$), which controls how fast this process should proceed. There&rsquo;s a couple caveat about how to set this hyperparameter: too big and there might be a chance your logreg model is going to overshoot its minimum and never converge. Or too small and your model takes forever to learn.</p>

<p>There&rsquo;s no hard and fast rule regarding how to set this hyperparameter appropriately. I will cover this topic in depth in a future post on model validation techniques.</p>

<p>The logistic regression model parameter update can be summarized as follows</p>

<pre><code>Repeat {
    w := w - learning_rate * (gradient w.r.t w)
    b := b - learning_rate * (gradient w.r.t b)
}
</code></pre>

<p>After this process ends, our final values for parameters $w$ and $b$ will be used to predict which class any unforeseen data case will fall into.</p>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

      </article>

      
      <span class="f6 gray mv3" title="Lastmod: April 26, 2019. Published at: 2019-04-26.">
        
      </span>

      
        <section class="mt4">
          
          
          <h3>ðŸ‘‹ Related posts in the <a href="/series/machine-learning-data-science-concepts" class="b bb bw1 pb1 no-underline black">Machine Learning &amp; Data Science concepts</a> series...</h3>
      
          
         
          
            <ul class="list pl0">
              
                <li class="list pl0 lh-copy">
                  <a
                    class="f4 b dib black no-underline"
                    href="https://thomaslam.github.io/post/deriving-cross-entropy-loss-gradient/"
                    >Deriving cross-entropy loss gradient</a
                  >
                  <span class="f6 gray">April 26, 2019</span>
                </li>
              
                <li class="list pl0 lh-copy">
                  <a
                    class="f4 b dib black no-underline"
                    href="https://thomaslam.github.io/post/machine-learning-an-overview/"
                    >Machine Learning: an overview</a
                  >
                  <span class="f6 gray">April 22, 2019</span>
                </li>
              
            </ul>  
          
        </section>
      

  </section>

  <footer>
    <div>
      <p class="f6 gray mt6 lh-copy">
        Â© 2016-19 <a href='https://github.com/siegerts/hugo-theme-basic'>Hugo Theme Basic</a>. Made by <a href='https://twitter.com/siegerts'>@siegerts</a>.
      </p>
    </div>
  </footer>
  
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.14.2/highlight.min.js"></script>

<script>
  hljs.initHighlightingOnLoad();
</script>



  </body>
</html>
