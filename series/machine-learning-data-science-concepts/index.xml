<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning &amp; Data Science concepts on Thomas Lam</title>
    <link>https://thomaslam.github.io/series/machine-learning-data-science-concepts/</link>
    <description>Recent content in Machine Learning &amp; Data Science concepts on Thomas Lam</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© 2016-19 &lt;a href=&#39;https://github.com/siegerts/hugo-theme-basic&#39;&gt;Hugo Theme Basic&lt;/a&gt;. Made by &lt;a href=&#39;https://twitter.com/siegerts&#39;&gt;@siegerts&lt;/a&gt;.</copyright>
    <lastBuildDate>Fri, 26 Apr 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://thomaslam.github.io/series/machine-learning-data-science-concepts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deriving cross-entropy loss gradient</title>
      <link>https://thomaslam.github.io/post/deriving-cross-entropy-loss-gradient/</link>
      <pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://thomaslam.github.io/post/deriving-cross-entropy-loss-gradient/</guid>
      <description>Note: This post assumes familiarity of vector notations and strong knowledge of (partial) derivatives which is covered in a Multivariable Calculus class.
In this blog post, I&amp;rsquo;m going to show how to derive the gradient (vector of partial derivatives) of the cost function for logistic regression (also known as cross entropy loss). This is an important part of backpropagation step/gradient descent algorithm in training logistic regression model (which I have explained in an earlier blog post) as in order to update the model parameters, we need to know the gradient of the cost function with respect to them.</description>
    </item>
    
    <item>
      <title>Logistic regression explained</title>
      <link>https://thomaslam.github.io/post/logistic-regression-explained/</link>
      <pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://thomaslam.github.io/post/logistic-regression-explained/</guid>
      <description>Logistic regression is one of the most famous ML models used in supervised learning settings. Despite the term &amp;ldquo;regression&amp;rdquo; in its name, this model is mostly used for classification tasks. An important distinction between logistic regression and another very popular supervised learning model, linear regression, is that the response variable is categorical for the former but continuous for the latter.
Most ML researchers use this model as a baseline in many supervised learning tasks because of its simplicity.</description>
    </item>
    
    <item>
      <title>Machine Learning: an overview</title>
      <link>https://thomaslam.github.io/post/machine-learning-an-overview/</link>
      <pubDate>Mon, 22 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://thomaslam.github.io/post/machine-learning-an-overview/</guid>
      <description>In this introductory post, I introduce what Machine Learning is in hopefully a widely accessible language. I aim to minimize as many buzzwords as possible though certain terminologies such as matrices, vectors, statistical models, parameters, etc. will come up. A basic understanding of what these terms mean is assumed.
What is Machine Learning? According to computer scientist Arthur Samuel in 1959, Machine Learning is a subfield of Computer Science that gives “computers the ability to learn without being explicitly programmed.</description>
    </item>
    
  </channel>
</rss>